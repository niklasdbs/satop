# @package _global_
experiment_name: "tian"
number_of_agents: 1
shared_agent : True
early_stopping: True
add_other_agents_targets_to_resource: False
add_current_position_of_other_agents: False #add the current position of other agents
add_route_positions: False #add a flag four every resource on the current route of an agent

#non agent specific
do_not_use_fined_status: False #treat fined resources as occupied
optimistic_in_violation: True
add_x_y_position_of_resource: True

observation: FullObservationGRCNSharedAgent
shared_reward: False
create_observation_between_steps: False
#replay_size : 100000 #100000
#model_optimizer: { optimizer: RMSprop ,lr: 0.002, alpha: 0.99}
#model_optimizer: { optimizer: Adam ,lr: 0.0012}
#model_optimizer: { optimizer: AdamW ,lr: 0.0012}
model_optimizer: { optimizer: Adam ,lr: 0.00001 }

#update_target_every : 3125 #3125 # 125 #3125
#slow_target_fraction : 1.0
#batch_size : 256 #256
max_gradient_norm : 2.0
train_steps: 1
replay_size: 512
batch_size: 512
max_sequence_length: 512
#start_learning: 5000
test_max_likelihood: True

critic_coefficient: 0.5
entropy_coefficient: 0.001

number_of_parallel_envs: 4

eval_every : 1600
evaluation_episodes: 1
save_every : 1600

#epsilon_initial: 1.0 #number of steps until the epsilon decay starts
#epsilon_min: 0.01 #minimum value of epsilon (e.g. 0.01)
#epsilon_decay_start: 10000 #number of steps until the epsilon decay starts
#steps_till_min_epsilon: 5000000 #5000000 #steps until the minimum epsilon value should be reached (e.g. 200.000)
#epsilon_decay: exp #exp or linear

defaults:
  - override /agent: ac
  - override /agent/model@model: time_at